[server]
url = "http://localhost:11434"     # Ollama server URL
api_key = "api-key"                # API key for OpenAI
model = "ollama:chat:llama3.2"     # Default model to use incase not multiple models
timeout = 600.0
models = ["ollama:chat:llama3.2:3b",
"openai:gpt-4o",
"ollama:chat:deepseek-r1:1.5b",
"ollama:chat:qwen:1.8b",
"ollama:chat:gemma2:2b",
"ollama:chat:phi3:3.8b",
"ollama:chat:mistral:7b",
"ollama:chat:wizardlm2"
]

[inference]
temperature = 0.1               # Sampling temperature is low for now for consistency in reproduction
max_tokens = 256
top_p = 0.1                     # system_prompt incorporated in models.py file

[log]
verbosity = 1
log_prompt = true
# very similar to the config file in parent dir (used as template)