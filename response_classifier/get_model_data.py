import threading
import requests
import json
import argparse
import time
from tqdm import tqdm
from collections import defaultdict, Counter
from queue import Queue


# Parse CLI arguments
parser = argparse.ArgumentParser(description="Collecting data for APIs")
parser.add_argument("--url", type=str, required=True, help="OpenAI-compatible API endpoint")
parser.add_argument("--api_key", type=str, required=True, help="OpenAI API key")
parser.add_argument("--model", type=str, required=True, help="Model to use for completion")
parser.add_argument("--temperature", type=float, default=0.0, help="Sampling temperature for the model")
parser.add_argument("--prompt", type=str, required=True, help="Static prompt to use for all requests")
args = parser.parse_args()

# Static prompt to use 80 times
STATIC_PROMPT = args.prompt
NUM_REQUESTS = 20

# Create a queue with the static prompt 80 times
prompt_queue = Queue()
for _ in range(NUM_REQUESTS):
    prompt_queue.put(STATIC_PROMPT)

# Initialize thread-safe data structures
lock = threading.Lock()
word_position_frequencies = defaultdict(lambda: defaultdict(int))  # {position: {word: count}}
failed_count = 0
MAX_RETRIES = 10


def extract_pipe_separated_words(generated_text):
    """
    Extract words that are wrapped in "|" symbols from the generated text.
    Handles formats like |word| or just words separated by |.
    
    :param generated_text: Full text generated by the LLM.
    :return: List of words extracted from pipe symbols or empty list if none found.
    """
    stripped_text = generated_text.strip()
    
    if "|" not in stripped_text:
        return []
    
    # Split by "|" and process each part
    parts = stripped_text.split("|")
    words = []
    
    for part in parts:
        # Clean up the part - remove extra whitespace, numbers, periods, etc.
        cleaned = part.strip()
        
        # Remove common prefixes like numbers and periods
        import re
        cleaned = re.sub(r'^\d+\.?\s*', '', cleaned)  # Remove leading numbers and periods
        cleaned = cleaned.strip()
        
        # More lenient filtering - accept words with letters (may include some punctuation)
        if (cleaned and 
            len(cleaned) > 1 and 
            re.search(r'[a-zA-Z]', cleaned)):  # Must contain at least one letter
            # Clean up common punctuation but keep the word
            cleaned = re.sub(r'[^\w\s-]', '', cleaned).strip()  # Keep letters, numbers, spaces, hyphens
            if cleaned:  # Final check after cleaning
                words.append(cleaned)
    
    return words


def get_response(progress_bar):
    global failed_count
    
    while not prompt_queue.empty():
        try:
            prompt = prompt_queue.get_nowait()
        except:
            return

        headers = {
            "Authorization": f"Bearer {args.api_key}",
            "Content-Type": "application/json"
        }


        payload = {
            "model": args.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0,
            "max_tokens": 512,  # Increased to allow for multiple words
        }

        try:
            response = requests.post(f"{args.url}/chat/completions", headers=headers, json=payload)
            response.raise_for_status()
            response_json = response.json()

            content = response_json["choices"][0]["message"]["content"].strip()
            words = extract_pipe_separated_words(content)
            print(content)
            print("words: "+str(words))
            # Debug: Print first few responses to see what's being extracted
            if progress_bar.n < 3:  # Only for first 3 responses
                progress_bar.write(f"Response content: {content[:200]}...")
                progress_bar.write(f"Extracted words: {words}")

            with lock:
                for position, word in enumerate(words):
                    word_position_frequencies[position][word] += 1
                failed_count = 0  # Reset failure count on success
                progress_bar.update(1)

        except Exception as e:
            print(f"Error: {e}")
            with lock:
                failed_count += 1

                if failed_count <= MAX_RETRIES:
                    # Add the prompt back to the queue for retry
                    prompt_queue.put(prompt)
                else:
                    progress_bar.write(f"Permanent failure on request")

            # Exponential delay (1s, 3s, 9s, 27s, .., 60s)
            delay = min(3 ** (failed_count - 1), 60)
            time.sleep(delay)


def run_threads(num_threads, progress_bar):
    threads = []

    for _ in range(num_threads):
        thread = threading.Thread(target=get_response, args=(progress_bar,))
        thread.start()
        threads.append(thread)

    for thread in threads:
        thread.join()


# Main execution
if __name__ == "__main__":
    print(f"Using static prompt: '{STATIC_PROMPT}'")
    print(f"Making {NUM_REQUESTS} requests")
    print("===== Starting data collection =====")

    # Create tqdm progress bar
    with tqdm(total=NUM_REQUESTS, desc="Collecting data", unit="req", mininterval=1.0, smoothing=0.05,
              bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} "
                         "[{elapsed}<{remaining}, {rate_fmt}{postfix}]") as progress_bar:

        while not prompt_queue.empty():
            batch_size = min(200, prompt_queue.qsize())
            run_threads(batch_size, progress_bar)

        progress_bar.set_postfix_str("Done!", refresh=True)

    print("All requests completed!")

    # Debug: Check if we have any data
    if not word_position_frequencies:
        print("WARNING: No words were extracted from any responses!")
        print("This might indicate an issue with the extraction function or API responses.")


    # Convert defaultdict to regular dict and calculate totals
    result_dict = {}
    total_words_by_position = {}
    
    for position, word_counts in word_position_frequencies.items():
        result_dict[f"position_{position}"] = dict(word_counts)
        total_words_by_position[position] = sum(word_counts.values())
    
    # Calculate overall statistics
    total_words_extracted = sum(total_words_by_position.values())
    unique_words_overall = set()
    for word_counts in word_position_frequencies.values():
        unique_words_overall.update(word_counts.keys())
    
    # Prepare final results
    final_results = {
        "prompt": STATIC_PROMPT,
        "total_requests": NUM_REQUESTS,
        "total_words_extracted": total_words_extracted,
        "unique_words_overall": len(unique_words_overall),
        "words_by_position": total_words_by_position,
        "word_frequencies_by_position": result_dict
    }

    # Generate filename
    if "/" in args.model:
        filename = f"./results/{args.model.split('/')[1]}_pipe_words_{args.temperature}.json"
    else:
        filename = f"./results/{args.model}_pipe_words_{args.temperature}.json"

    with open(filename, "w") as f:
        json.dump(final_results, f, indent=2)

    print(f"Results saved to {filename}")
    print(f"Total words extracted: {total_words_extracted}")
    print(f"Unique words overall: {len(unique_words_overall)}")
    print(f"Words extracted by position: {total_words_by_position}")
    
    # Show most common words for each position
    for position in sorted(word_position_frequencies.keys()):
        word_counts = word_position_frequencies[position]
        if word_counts:
            most_common = Counter(word_counts).most_common(3)
            print(f"Position {position} most common: {most_common}")